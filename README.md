# AIApplicationEngineer
AIApplicationEngineer 人工智能应用工程师

## 引言
在这个快速发展的人工智能时代，在ChatGPT Deepseek等大模型快速退出，并能应用解决现实世界中的问题的时候,我们每个人都应该使用AI工具为自己赋能，如冷兵器时代的弓箭，热兵器时代的火枪，御敌千米之外。本教程旨在为初学者和中级开发者提供一个全面的学习路径，从基础概念到高级实现技巧。
## 1. LLM的分类
AI模型的分类确实非常多样，每种类型的模型都有其独特的功能和应用场景。下面我将详细介绍几种常见的AI大模型类别,文本处理模型,图像生成模型,语音转文字模型,
嵌入模型（Embedding Models）,多模态模型,也需要对这些模型进行一些了解。
### 1.1 自然语言模型-NLP模型
这类模型主要处理和理解人类语言，包括文本生成、翻译、情感分析、文本摘要、问答系统等。典型的ChatGPT和Deepseek。
### 1.2 图像生成模型
OpenAI的DALL-E
DALL-E 是一个基于变换器的神经网络模型，能够生成与给定文本描述相匹配的图像。它的名字来源于艺术家萨尔瓦多·达利和PIXAR的动画电影《机器人总动员》（WALL-E）。
Google的Imagen
Imagen 是Google推出的一个高分辨率图像生成模型，它使用了称为Transformer的文本编码器和扩散模型来生成图像。Imagen以其生成图像的高质量和细节而闻名。
Stable Diffusion
Stable Diffusion 是一个开源的文生图模型，它使用扩散模型来生成图像。这个模型因其开放性和可定制性受到了广泛的欢迎，社区活跃，应用也非常广泛。
### 1.3 语音转文字模型
microsoft的speecht5_tts
Microsoft 的 SpeechT5_TTS 模型是一个基于 Transformer 的文本到语音（TTS）模型，它是 SpeechT5 模型家族的一部分。这个模型利用了自然语言处理中的最新技术，特别是 Transformer 架构，来生成高质量、自然 sounding 的语音。
### 1.4 嵌入模型 Embedding模型
嵌入式模型的核心思想是将每个数据点映射到一个低维度的嵌入向量（embedding vector）中，使得相似的数据点在嵌入空间中距离接近。
Contriever, OpenAI Embeddings, BAAI BGE等方法采用多阶段训练策略。
### 1.5 多模态模型
多模态模型是指能够处理和理解多种类型数据（如文本、图像、声音等）的模型。这些模型通过整合来自不同模态的信息，能够提供更为丰富和准确的分析和预测。
NExT-GPT
这是一种端到端、通用且支持任意模态到任意模态的 MM-LLM，支持自由输入和输出图像、视频、音频和文本。其采用了一种轻量的对齐策略 —— 在编码阶段使用以 LLM 为中心的对齐，在解码阶段使用指令遵从对齐。
Video-LLaMA
提出了一种多分支跨模态预训练框架，让 LLM 可以在与人类对话的同时处理给定视频的视觉和音频内容。该框架对齐了视觉与语言以及音频与语言。


## 2. 用好大模型
经常会遇到有些人有反馈 大模型不好用,大模型回答的不正确,答非所问浪费我的时间。。。
这是LLM准确度的一个问题。如何在使用 LLM 时最大限度地提高正确性和一致性行为？
本文给出了一个关于如何优化 LLM 以提高准确性和行为的思维模型。我们将探讨诸如提示工程、RAG检索增强生成 和微调等方法。

![LLM优化矩阵](Attach/llm_op.png)
<center>图2-1 LLM优化矩阵</center>

典型的 LLM 任务将从左下角的快速工程开始，我们在此进行测试、学习和评估以获得基准。
- 上下文优化的场景
1. 模型缺乏上下文知识（不在训练集中）
2. 知识已过时
3. 专有信息知识
- LLM 优化场景
1. 模型产生的结果不一致且格式不正确
2. 语气或说话风格不正确
3. 推理没有遵循正确逻辑

## 3 提示工程指南
### 3.1 提示工程背景
提示工程（Prompt Engineering）是一门较新的学科，关注提示词开发和优化，帮助用户将大语言模型（Large Language Model, LLM）用于各场景和研究领域。 掌握了提示工程相关技能将有助于用户更好地了解大型语言模型的能力和局限性。提示工程主要是在理解大语言模型能力的基础上设计和研发提示词。
在开始前我们先从几个模型设置参数了解一下大语言模型的生成规则。不同的客户端参数配置在不同的地方，常见的是大模型上有个设置按钮。
1. Temperature,范围0-1,temperature 的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。比如专业知识问答需要固定确定的答案需要调小temperature,写作创作等灵活问答需要更多创造性的答案建议调大temperature。
2. Top_p,范围0-1,与temperature类似,Top P意味着只有词元集合（tokens）中包含top_p概率质量的才会被考虑用于响应,与 temperature 一起称为核采样（nucleus sampling）的技术。
一般建议是改变 Temperature 和 Top P 其中一个参数就行，不用两个都调整。
3. Max Length,max length来控制大模型生成的 token 数，设置合适的max length有助于防止大模型生成冗长或不相关的响应。
4. Stop Sequences,字符串,stop sequences 是控制大模型响应长度和结构的一种方式,比如生成项目列表有12项，我们只需要其中的10项,设置11为stop sequences,在生成11的时候就会自动停止生成。
5. Frequency Penalty,字符串,设置再次出现的可能性就越小。比如生成的回答中出现了大量重复的词，我们又不需要可以设置该词减少重复的输出。
6. Presence Penalty,字符串,与Frequency Penalty类似。都是对生成重复的词进行惩罚,不同的是惩罚规则不同,Presence Penalty会对出现2次和出现10次是相同的惩罚。一般建议是改变frequency penalty和 presence penalty 其中一个参数就行，不要同时调整两个。

提示词可以包含以下任意要素：
指令：想要模型执行的特定任务或指令。
上下文：包含外部信息或额外的上下文信息，引导语言模型更好地响应。
输入数据：用户输入的内容或问题。
输出指示：指定输出的类型或格式。

提示词：
问题:  介绍一下prompt。
输出：
答案: 在计算机科学和人工智能领域，特别是在与像我这样的语言模型交互时，"prompt"指的是用户输入的文本，这些文本旨在引导或激发模型生成特定的回答或输出。
例如，当你问我“介绍一下prompt”时，这个问题本身就是一个prompt，它指导我如何回答你的问题。在编程和脚本语言中，prompt也可以指命令行界面中等待用户输入的文本提示符。总的来说，prompt是一种工具，用于引导对话或操作的方向。
### 3.2 提示工程技巧
从开始设计提示词的时候,实际上是一个迭代过程，需要大量的实验才能获得最佳结果。
#### 3.2.1. 更准确清晰的说明
一段简短的文字描述其意含糊不清，往往大模型给出的答案不令人满意，这样的情况下我们主要使用 背景-现状-要求的方式进行提示。
| Bad | Good     |
|    :----:   |          :---: |
| 如何在Excel中添加数字累加和 | 我要一张Excel表,B列为总费用,我现在需要对B2到B9的数据进行总计求和的操作,给出完整的excel公式 |
| 总结会议记录 | 用一段话总结会议记录,然后写下演讲者及其要点的简要列表。最后，列出演讲者的总结和建议或者后续的行动 |
| 编写代码计算斐波那契数列 | 编写一个TypeScript函数来高效计算斐波那契数列。对代码进行大量注释，解释每个部分的作用以及为什么这样编写。 |

#### 3.2.2. 提供参考文本
| Bad | Good     |
|    :----:   |          :---: |
| 我需要把excel里面的一列格式月.日修改为 2024.月.日 给出公式 | 我需要把excel里面的一列格式月.日修改为 2024.月.日,比如 6.30->2024.6.30,7.1->2024.7.1,给完整的excel公式 |

#### 3.2.3. 补充模型应该的角色
可以使用三个不同的角色来构建prompt：system、user和assistant。其中system不是必需的，但有助于设定assistant的整体行为，帮助模型了解用户的需求，并根据这些需求提供相应的响应。

| Bad | Good     |
|    :----:   |          :---: |
| system:你是一位有用的助手 | 你是一位专业到且仔细的财务助手。您的目标是深入了解用户的意图，在回答用户费用问题时候，逐步思考复杂问题，提供清晰准确的答案，并主动预测有用的后续信息。|

#### 3.2.4. 指定完成任务所需的步骤
有些任务最好以一系列步骤的形式来描述。明确地写出这些步骤可以让模型更容易遵循它们。
| Bad | Good     |
|    :----:   |          :---: |
| 我有一个月.日的时间格式需要进行加3个月的转换并输出转换后的时间,数据为 7.27 5.21 ... | 我有一个月.日的时间格式,需要第一步补充年的时间为 2024,第二步在转换后的时间格式基础上(年.月.日)上计算增加3个月,输出转换后的时间格式为 新的时间为: ,数据为 7.27 5.21 ... |

#### 3.2.5. 指定需要输出长度
您可以要求模型生成具有给定目标长度的输出。目标输出长度可以根据单词、句子、段落、要点等的数量来指定。
| Bad | Good     |
|    :----:   |          :---: |
| 请总结下面的文本内容。 xxx | 请用一段话总结下面的文本100字以内。xxx |
| 请总结下面的文本内容。 xxx | 请用简洁的语言对文本进行总结并最后一句话总结核心观点。xxx |

#### 3.2.6. 总结之前的对话
由于模型具有固定的上下文长度，因此用户和助手之间的对话，对于长对话内容，大预言模型对之前的内容可能被截断漏掉导致丢失了部分有效信息，一个比较好的方式是在一定交互后对之前的对话进行总结，这样启动总结有用信息过滤无用信息的作用。

| Bad | Good     |
|    :----:   |          :---: |
| 多轮会话继续提问 | 多轮会话后进行总结后再提问 |

#### 3.2.7. 其他
还有很多其他的方式,比如指导模型按思维链的方式进行逐步思考，将复杂任务拆分为子任务等，具体可以参考下面的链接
https://platform.openai.com/docs/guides/prompt-engineering#tactic-specify-the-desired-length-of-the-output

## 4. RAG检索增强生成
### 4.1 背景
RAG 是在生成答案之前检索内容以补充LLM 提示的过程。它用于使模型能够访问特定领域的上下文来解决任务。RAG 是一种非常有价值的工具，可以提高LLM的准确性和一致性。

![RAG典型工作流程图](Attach/rag.png)
<center>图4-1 RAG典型工作流程图</center>

RAG能通过补充相关信息的上下文来提供更有效的搜索，能很好的解决信息过时或者信息没有的问题。比如外挂知识库,进行联网搜索。
但是RAG也面临一下问题,如何补充正确的相关的上下文信息？相关信息的上下文太多导致淹没了真实的信息，相关信息太少导致无法检索到所有有关信息，在此基础上RAG也经历了从初级RAG到高级RAG到模块化RAG的演进。本文就RAG演进进行详细说明。
![RAG演进](Attach/rag_evolution.png)
<center>图4-2 RAG演进图</center>

### 4.2 初级RAG
初级RAG采用了一个传统过程，其核心主要包括检索、生成和增强三大环节。
检索: 首先是将数据分块建立索引，系统根据用户的输入，根据关键字查询相关索引文档。
生成: 系统将这些文档和一个提示语结合起来,交给模型生成回答。
增强: 有了相关上下文的信息加成，模型能有效的提高准确性和可靠性。
因此检索相关文档的质量至关重要，而初期检索相关文档大都采用关键词相识度检索，导致了低精确度（检索到的信息不够准确）和低召回率（有时候无法检索到所有相关的信息）,从而影响回答的准确性和可靠性。

### 4.3 高级RAG
高级RAG要解决初级RAG 面临的问题，主要是提高检索质量方面，包括优化检索前、检索时和检索后的各个过程。
在检索前的准备阶段，我们通过优化数据的索引建立来提高数据质量，包括改善数据的细节度、优化索引结构、添加元数据、改进对齐方式以及混合检索方法。在检索阶段，我们可以通过改进嵌入模型来提高上下文片段的质量。例如，通过对嵌入模型进行微调，以提高检索的相关性，或者使用能够更好理解上下文的动态嵌入模型（如OpenAI的 embeddings-ada-02模型），或者使用向量数据库,从关键词相似度转向语义相识度，做到更好的理解和匹配关键信息。
在检索后的优化过程中，我们专注于解决上下文窗口限制和减少噪音或分散注意力的信息。常用的方法包括重新排列文档，以将更相关的内容放在提示的前后，或者重新计算查询与文档片段之间的语义相似度。如果涉及多轮会话，上下文过多会进行总结概括，最后进行融合在prompt中给到LLM这样返回的准确性和可靠性有很大的改善。

此外还有模块化RAG，通过增强其功能模块来提升性能，例如加入相似性检索的搜索模块，以及在检索工具上进行精细调整。模块化RAG 能够根据具体的任务需求，添加、替换或调整模块之间的工作流程，从而实现更高的多样性和灵活性。这种设计让模块化RAG 不仅包括了朴素RAG和高级RAG这两种固定模式，还扩展了包括搜索、记忆、融合、路由、预测和任务适配等多种模块，以解决各种问题。
随着RAG系统构建变得更加灵活,目前出现一些以下几种方法。
混合式搜索探索： 结合了关键词搜索与语义搜索等多种搜索技术，以便检索到既相关又富含上下文的信息，特别适用于处理多样化的查询类型和信息需求。
递归式检索与查询引擎： 通过从小的语义片段开始，逐步检索更大的内容块以丰富上下文的递归过程，有效平衡了检索效率与信息的丰富度。
假设性文档嵌入技术 (HyDE)： 通过生成查询的假设性回答并嵌入，来检索与这个假设回答相似的文档，而不是直接使用查询本身，以此来优化检索效果。

RAG关键优化点:
- 提升语义理解
1. 数据分块策略： 确定合适的数据分块方式非常关键，这依赖于你的数据内容和应用需求。实际应用中，通过尝试不同的数据分块策略来发现最优的检索效率是常见做法。
2. 专业领域的嵌入模型微调： 确定了有效的数据分块策略后，如果你的工作聚焦于特定领域，可能还需要对嵌入模型进行微调。例如，BGE-large-EN开发的BAAI模型，就能通过微调来提高检索的相关性。
- 查询与文档的精准匹配
1. 查询重写： 通过多种技术改写查询，以提高匹配的准确性，例如利用Query2Doc、ITER-RETGEN和HyDE等工具。
2. 查询嵌入的优化： 通过调整查询的嵌入表示，使其更好地与任务相关的潜在空间对齐，从而提升查询效果。
- 文本生成
1. 检索后简化处理与优先排序: 在保持大语言模型(LLM)不变的情况下，通过后处理技术改善检索结果的质量，如通过信息简化和结果优先排序等手段。信息简化有助于减少冗余信息，解决模型处理长文本的限制，并提升最终文本的生成质量。优先排序则是将最相关的信息排在前面，以提高检索的准确性。
2. 针对RAG系统的LLM微调: 为了提高RAG系统的效率，可以对生成文本的过程进行细致调整或微调，确保生成的文本既自然流畅又能有效地结合检索到的文档信息。
- 增强
1. 高质量的数据源： 高质量的原始数据对检索至关重要。
2. 增强过程： 对于一些需要多步骤推理的问题，单次检索可能不足以解决，因此提出了以下几种方法：
2.1 迭代检索：模型通过多轮检索，不断深化和丰富信息内容。例如，RETRO 和 GAR-meets-RAG 就是采用这种方法。
2.2 递归检索：在这种方法中，一次检索的输出成为另一次检索的输入，逐步深入挖掘复杂查询的相关信息，适用于学术研究和法律案例分析等场景。著名实践包括 IRCoT 和 Tree of Clarifications。
2.3 自适应检索：根据特定需求调整检索过程，选择最合适的时机和内容进行检索，以达到最佳效果。这种方法的代表性研究包括 FLARE 和 Self-RAG。

## 5. 微调
### 
为了解决学习记忆问题，许多开发人员会在较小的特定领域数据集上继续LLM的训练过程，以针对特定任务进行优化。这个过程称为微调。
进行微调通常出于以下两个原因之一：
提高特定任务的模型准确性：通过向模型展示许多正确执行该任务的示例，对特定任务数据进行训练，以解决学习记忆问题。
提高模型效率：用更少的token或者更小的模型达到同样的准确率。
微调过程从准备训练示例数据集开始 - 这是最关键的一步，因为您的微调示例必须准确地代表模型在现实世界中看到的内容。
![微调示意图](Attach/fine-tuning.png)
<center>图4-1 微调示意图</center>
获得此干净的数据集后，您可以通过执行训练运行来训练微调模型-根据您用于训练的平台或框架，您可能有可以在此处调整的超参数，与任何其他机器学习模型类似。我们始终建议在训练后保留一个保留集以用于评估，以检测过度拟合。有关如何构建良好训练集的提示，您可以查看我们的微调文档中的指导。训练完成后，新的微调模型即可用于推理。

## 6. RAG综合实战
### 6.1 
